{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a368734",
   "metadata": {},
   "source": [
    "# The Audit Planning Agent - Draft Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e956d0c",
   "metadata": {},
   "source": [
    "## Stage 1 - Ingestion\n",
    "Ingesting trial balance before mapping into a full fledge financial report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8daf0a3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "\n",
    "# Define data folder and input files\n",
    "data_folder = Path.cwd().parent / \"data\"\n",
    "tb_2024_path = data_folder / \"TB_2024.xlsx\"\n",
    "tb_2025_path = data_folder / \"TB_2025.xlsx\"\n",
    "\n",
    "# Ingest prior-year and current-year trial balance\n",
    "tb_2024 = pd.read_excel(tb_2024_path)\n",
    "tb_2025 = pd.read_excel(tb_2025_path)\n",
    "\n",
    "# Display the first few rows of the trial balance\n",
    "print(\"2024 Trial Balance:\")\n",
    "display(tb_2024.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbbbdad",
   "metadata": {},
   "source": [
    "## Stage 2 - Mapping to Leadsheet\n",
    "### Normalize input and Detect leaf accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69a3c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VAS mapping file\n",
    "vas_mapping_path = Path.cwd().parent / \"Mapping VAS.xlsx\"\n",
    "vas_mapping = pd.read_excel(vas_mapping_path, sheet_name=\"Sheet1\")\n",
    "display(vas_mapping.head())\n",
    "\n",
    "# Normalizes trial balance data by reusing tb_2024/tb_2025 from Stage 1 with added period labels,\n",
    "# standardizes account_id fields through trimming / optional digit stripping, and loads the VAS mapping\n",
    "# file while trimming the prefix column and retaining only necessary mapping columns for subsequent processing.\n",
    "# --- Normalize and concatenate TBs with period labels ---\n",
    "ACCOUNT_ID_CANDIDATES = (\"account_id\", \"Account No\", \"AccountNo\", \"account no\", \"Account_no\")\n",
    "\n",
    "def normalize_account_id(x: str, digits_only: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Normalize account_id as character/text. \n",
    "    - Trims whitespace.\n",
    "    - Optionally removes non-digits if digits_only=True.\n",
    "    Always returns string.\n",
    "    \"\"\"\n",
    "    s = str(x).strip().replace(\" \", \"\")\n",
    "    if digits_only:\n",
    "        s = \"\".join(c for c in s if c.isdigit())\n",
    "    return s\n",
    "\n",
    "\n",
    "def ensure_account_id(df: pd.DataFrame, *, digits_only: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Ensure the dataframe exposes an 'account_id' column using common fallback names.\"\"\"\n",
    "    for candidate in ACCOUNT_ID_CANDIDATES:\n",
    "        if candidate in df.columns:\n",
    "            df = df.copy()\n",
    "            if candidate != \"account_id\":\n",
    "                df[\"account_id\"] = df[candidate]\n",
    "            df[\"account_id\"] = df[\"account_id\"].apply(lambda x: normalize_account_id(x, digits_only=digits_only))\n",
    "            return df\n",
    "    raise KeyError(\"No account identifier column found in trial balance dataframe.\")\n",
    "\n",
    "\n",
    "tb_2024_norm = ensure_account_id(tb_2024)\n",
    "tb_2025_norm = ensure_account_id(tb_2025)\n",
    "\n",
    "for df, period in ((tb_2024_norm, \"2024\"), (tb_2025_norm, \"2025\")):\n",
    "    df[\"period\"] = period\n",
    "\n",
    "tb_all = pd.concat([tb_2024_norm, tb_2025_norm], ignore_index=True)\n",
    "# tb_all now has standardized account_id values and a \"period\" label\n",
    "\n",
    "\n",
    "def normalize_prefix(value: str) -> str | None:\n",
    "    \"\"\"Normalize mapping prefixes (handle floats like 112.0 -> '112', strip whitespace).\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    s = str(value).strip()\n",
    "    if s == \"\":\n",
    "        return None\n",
    "    # remove decimal part for numeric-looking prefixes and strip non-digit separators\n",
    "    if \".\" in s:\n",
    "        s = s.split(\".\", 1)[0]\n",
    "    s = \"\".join(c for c in s if c.isdigit())\n",
    "    return s or None\n",
    "\n",
    "\n",
    "mapping_columns = ['1st', 'Leadsheet', 'Item on FSs', 'FSs code']\n",
    "vas_mapping = vas_mapping[mapping_columns].copy()\n",
    "vas_mapping = vas_mapping.rename(columns={'1st': 'prefix'})\n",
    "vas_mapping['prefix'] = vas_mapping['prefix'].apply(normalize_prefix)\n",
    "vas_mapping = vas_mapping.dropna(subset=['prefix'])\n",
    "vas_mapping = vas_mapping[vas_mapping['prefix'] != \"\"]\n",
    "\n",
    "# Detect leaf account per period (terminal nodes in account hierarchy).\n",
    "def compute_leaf_flags(series: pd.Series) -> pd.Series:\n",
    "    codes = series.astype(str).tolist()\n",
    "    leaf_mask = []\n",
    "    for code in codes:\n",
    "        leaf_mask.append(not any(other != code and other.startswith(code) for other in codes))\n",
    "    return pd.Series(leaf_mask, index=series.index)\n",
    "\n",
    "# Apply per period using transform to preserve original order without groupby apply warnings.\n",
    "tb_all['is_leaf'] = tb_all.groupby('period')['account_id'].transform(compute_leaf_flags)\n",
    "# tb_all now has is_leaf boolean column per period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e102713",
   "metadata": {},
   "source": [
    "Map leaves to leadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc4cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Prefix-based leaf account mapping, longest-prefix-first, vectorized --\n",
    "\n",
    "# Work only on leaf accounts\n",
    "tb_leaves = tb_all[tb_all['is_leaf']].copy()\n",
    "\n",
    "# Sort vas_mapping by descending prefix length (most specific first)\n",
    "vas_mapping['prefix_len'] = vas_mapping['prefix'].str.len()\n",
    "vas_mapping_sorted = vas_mapping.sort_values('prefix_len', ascending=False).reset_index(drop=True)\n",
    "prefix_tuples = list(zip(vas_mapping_sorted['prefix'], \n",
    "                         vas_mapping_sorted['Leadsheet'], \n",
    "                         vas_mapping_sorted['Item on FSs'], \n",
    "                         vas_mapping_sorted['FSs code']))\n",
    "\n",
    "def longest_prefix_lookup(acc_id: str):\n",
    "    acc_id_str = str(acc_id)\n",
    "    for prefix, leadsheet, item_on_fs, fs_code in prefix_tuples:\n",
    "        if acc_id_str.startswith(prefix):\n",
    "            return {\n",
    "                \"matched_prefix\": prefix,\n",
    "                \"Leadsheet\": leadsheet,\n",
    "                \"Item on FSs\": item_on_fs,\n",
    "                \"FSs code\": fs_code,\n",
    "                \"map_method\": \"prefix\",\n",
    "            }\n",
    "    # No prefix found\n",
    "    return {\n",
    "        \"matched_prefix\": None,\n",
    "        \"Leadsheet\": None,\n",
    "        \"Item on FSs\": None,\n",
    "        \"FSs code\": None,\n",
    "        \"map_method\": None,\n",
    "    }\n",
    "\n",
    "# Apply mapping to each leaf account (vectorized via DataFrame.apply)\n",
    "leaf_map_df = tb_leaves['account_id'].apply(longest_prefix_lookup).apply(pd.Series)\n",
    "tb_leaves = tb_leaves.join(leaf_map_df)\n",
    "\n",
    "# Collect unmapped leaf accounts for review\n",
    "unmapped_leaves = tb_leaves[tb_leaves['matched_prefix'].isnull()].copy()\n",
    "\n",
    "display(unmapped_leaves)\n",
    "display(tb_leaves.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b1b0e0",
   "metadata": {},
   "source": [
    "## Stage 3 - Financial Statement Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb674531",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# --- Stage 3 · Prepare measures on mapped leaf data ---\n",
    "tb_leaves_measured = tb_leaves.copy()\n",
    "amount_source_columns = OrderedDict(\n",
    "    [\n",
    "        (\"dr\", \"Dr\"),\n",
    "        (\"cr\", \"Cr\"),\n",
    "        (\"opening_dr\", \"Opening Dr\"),\n",
    "        (\"opening_cr\", \"Opening Cr\"),\n",
    "        (\"closing_dr\", \"Closing Dr\"),\n",
    "        (\"closing_cr\", \"Closing Cr\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "for alias, source_col in amount_source_columns.items():\n",
    "    if source_col in tb_leaves_measured.columns:\n",
    "        tb_leaves_measured[source_col] = tb_leaves_measured[source_col].fillna(0.0)\n",
    "        tb_leaves_measured[alias] = tb_leaves_measured[source_col]\n",
    "    else:\n",
    "        tb_leaves_measured[alias] = 0.0\n",
    "\n",
    "tb_leaves_measured[\"bs_amount\"] = tb_leaves_measured[\"closing_dr\"] - tb_leaves_measured[\"closing_cr\"]\n",
    "tb_leaves_measured[\"pl_amount\"] = tb_leaves_measured[\"dr\"] - tb_leaves_measured[\"cr\"]\n",
    "tb_leaves_measured[\"opening_amount\"] = (\n",
    "    tb_leaves_measured[\"opening_dr\"] - tb_leaves_measured[\"opening_cr\"]\n",
    ")\n",
    "\n",
    "# Filter to mapped rows only (retain unmatched separately for diagnostics).\n",
    "mapped_leaves = tb_leaves_measured[tb_leaves_measured[\"FSs code\"].notna()].copy()\n",
    "mapped_leaves[\"FSs code\"] = mapped_leaves[\"FSs code\"].astype(str).str.strip()\n",
    "mapped_leaves = mapped_leaves[mapped_leaves[\"FSs code\"] != \"\"]\n",
    "\n",
    "# --- Stage 3 · Aggregate by financial statement code ---\n",
    "group_keys = [\"period\", \"FSs code\", \"Leadsheet\", \"Item on FSs\"]\n",
    "fs_bs = (\n",
    "    mapped_leaves.groupby(group_keys, dropna=False, as_index=False)[\"bs_amount\"].sum()\n",
    ")\n",
    "fs_pl = (\n",
    "    mapped_leaves.groupby(group_keys, dropna=False, as_index=False)[\"pl_amount\"].sum()\n",
    ")\n",
    "\n",
    "# Helper to pivot statement data wide by period (rows keyed by FS code).\n",
    "def pivot_statement(df: pd.DataFrame, value_col: str, value_label: str) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"FSs code\", value_label])\n",
    "\n",
    "    value = (\n",
    "        df.pivot_table(\n",
    "            index=[\"FSs code\", \"Leadsheet\", \"Item on FSs\"],\n",
    "            columns=\"period\",\n",
    "            values=value_col,\n",
    "            aggfunc=\"sum\",\n",
    "            fill_value=0.0,\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values(\"FSs code\")\n",
    "    )\n",
    "    value.columns = [\n",
    "        \"FSs code\",\n",
    "        \"Leadsheet\",\n",
    "        \"Item on FSs\",\n",
    "        *[f\"{value_label}_{col}\" for col in value.columns[3:]],\n",
    "    ]\n",
    "    return value\n",
    "\n",
    "bs_pivot = pivot_statement(fs_bs, \"bs_amount\", \"bs_amount\")\n",
    "pl_pivot = pivot_statement(fs_pl, \"pl_amount\", \"pl_amount\")\n",
    "\n",
    "# --- Stage 3 · Optional template alignment ---\n",
    "artifacts_dir = Path.cwd().parent / \"artifacts\"\n",
    "artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "balance_sheet_output_path = artifacts_dir / \"fs_balance_sheet_pivot.xlsx\"\n",
    "income_statement_output_path = artifacts_dir / \"fs_income_statement_pivot.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(balance_sheet_output_path, engine=\"xlsxwriter\") as writer:\n",
    "    bs_pivot.to_excel(writer, sheet_name=\"BalanceSheet\", index=False)\n",
    "\n",
    "with pd.ExcelWriter(income_statement_output_path, engine=\"xlsxwriter\") as writer:\n",
    "    pl_pivot.to_excel(writer, sheet_name=\"IncomeStatement\", index=False)\n",
    "\n",
    "print(f\"Balance sheet pivot exported to {balance_sheet_output_path}\")\n",
    "print(f\"Income statement pivot exported to {income_statement_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Planning_Agent_NDD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
